{
  "train_losses": [
    0.12545516856730754,
    0.1082023090544849,
    0.1070406006822685,
    0.10503315962715974
  ],
  "val_losses": [
    0.1132905519328238,
    0.10949396343820327,
    0.10940756779189521,
    0.10812464590461751
  ],
  "learning_rates": [
    1e-05,
    1e-05,
    1e-05,
    1.0000000000000002e-06
  ],
  "train_metrics": [
    {
      "accuracy": 0.8673992673992674,
      "f1_score": 0.9414635277678448,
      "precision": 0.9849296006933006,
      "recall": 0.9045345887602657,
      "loss": 0.12545516856730754,
      "classification_report": {
        "legal_aid": {
          "precision": 0.9480178224608705,
          "recall": 0.9551654676258993,
          "f1-score": 0.9515782231014019,
          "support": 17375.0
        },
        "family_law": {
          "precision": 0.9879640617053738,
          "recall": 0.8141939089131042,
          "f1-score": 0.8927012330550662,
          "support": 7158.0
        },
        "consumer_protection": {
          "precision": 0.9981920449979912,
          "recall": 0.8414902624894157,
          "f1-score": 0.91316732518607,
          "support": 5905.0
        },
        "employment_law": {
          "precision": 0.9954885097983928,
          "recall": 0.9863109372817432,
          "f1-score": 0.9908784731967443,
          "support": 7159.0
        },
        "fundamental_rights": {
          "precision": 0.9949855645038748,
          "recall": 0.925512367491166,
          "f1-score": 0.9589923842999414,
          "support": 7075.0
        },
        "micro avg": {
          "precision": 0.9749149963145255,
          "recall": 0.9178456303724928,
          "f1-score": 0.9455199529569117,
          "support": 44672.0
        },
        "macro avg": {
          "precision": 0.9849296006933006,
          "recall": 0.9045345887602657,
          "f1-score": 0.9414635277678448,
          "support": 44672.0
        },
        "weighted avg": {
          "precision": 0.9760970083087714,
          "recall": 0.9178456303724928,
          "f1-score": 0.9445390897286073,
          "support": 44672.0
        },
        "samples avg": {
          "precision": 0.9769688644688644,
          "recall": 0.9309279609279609,
          "f1-score": 0.9460286063143205,
          "support": 44672.0
        }
      },
      "confusion_matrix": null
    },
    {
      "accuracy": 0.8811355311355311,
      "f1_score": 0.9500356715981141,
      "precision": 0.9899292883211679,
      "recall": 0.9160998242574105,
      "loss": 0.1082023090544849,
      "classification_report": {
        "legal_aid": {
          "precision": 0.9496464416058394,
          "recall": 0.9584460431654677,
          "f1-score": 0.9540259517057661,
          "support": 17375.0
        },
        "family_law": {
          "precision": 1.0,
          "recall": 0.8232746577256217,
          "f1-score": 0.9030725614895411,
          "support": 7158.0
        },
        "consumer_protection": {
          "precision": 1.0,
          "recall": 0.8584250635055038,
          "f1-score": 0.9238199380353563,
          "support": 5905.0
        },
        "employment_law": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 7159.0
        },
        "fundamental_rights": {
          "precision": 1.0,
          "recall": 0.9403533568904594,
          "f1-score": 0.9692599067599068,
          "support": 7075.0
        },
        "micro avg": {
          "precision": 0.9791302292602222,
          "recall": 0.9273594197707736,
          "f1-score": 0.9525419052217701,
          "support": 44672.0
        },
        "macro avg": {
          "precision": 0.9899292883211679,
          "recall": 0.9160998242574105,
          "f1-score": 0.9500356715981141,
          "support": 44672.0
        },
        "weighted avg": {
          "precision": 0.9804151800434604,
          "recall": 0.9273594197707736,
          "f1-score": 0.9516490168439949,
          "support": 44672.0
        },
        "samples avg": {
          "precision": 0.9838278388278389,
          "recall": 0.9405677655677656,
          "f1-score": 0.95498778998779,
          "support": 44672.0
        }
      },
      "confusion_matrix": null
    },
    {
      "accuracy": 0.8814285714285715,
      "f1_score": 0.9500781330324477,
      "precision": 0.9899230550014249,
      "recall": 0.9161919105883458,
      "loss": 0.1070406006822685,
      "classification_report": {
        "legal_aid": {
          "precision": 0.9496152750071245,
          "recall": 0.9589064748201439,
          "f1-score": 0.9542382588774342,
          "support": 17375.0
        },
        "family_law": {
          "precision": 1.0,
          "recall": 0.8232746577256217,
          "f1-score": 0.9030725614895411,
          "support": 7158.0
        },
        "consumer_protection": {
          "precision": 1.0,
          "recall": 0.8584250635055038,
          "f1-score": 0.9238199380353563,
          "support": 5905.0
        },
        "employment_law": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 7159.0
        },
        "fundamental_rights": {
          "precision": 1.0,
          "recall": 0.9403533568904594,
          "f1-score": 0.9692599067599068,
          "support": 7075.0
        },
        "micro avg": {
          "precision": 0.9791110375954063,
          "recall": 0.9275385028653295,
          "f1-score": 0.9526272832821786,
          "support": 44672.0
        },
        "macro avg": {
          "precision": 0.9899230550014249,
          "recall": 0.9161919105883458,
          "f1-score": 0.9500781330324477,
          "support": 44672.0
        },
        "weighted avg": {
          "precision": 0.9804030579165649,
          "recall": 0.9275385028653295,
          "f1-score": 0.9517315928895655,
          "support": 44672.0
        },
        "samples avg": {
          "precision": 0.9838095238095238,
          "recall": 0.9407142857142857,
          "f1-score": 0.9550793650793651,
          "support": 44672.0
        }
      },
      "confusion_matrix": null
    },
    {
      "accuracy": 0.8814285714285715,
      "f1_score": 0.9500781330324477,
      "precision": 0.9899230550014249,
      "recall": 0.9161919105883458,
      "loss": 0.10503315962715974,
      "classification_report": {
        "legal_aid": {
          "precision": 0.9496152750071245,
          "recall": 0.9589064748201439,
          "f1-score": 0.9542382588774342,
          "support": 17375.0
        },
        "family_law": {
          "precision": 1.0,
          "recall": 0.8232746577256217,
          "f1-score": 0.9030725614895411,
          "support": 7158.0
        },
        "consumer_protection": {
          "precision": 1.0,
          "recall": 0.8584250635055038,
          "f1-score": 0.9238199380353563,
          "support": 5905.0
        },
        "employment_law": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 7159.0
        },
        "fundamental_rights": {
          "precision": 1.0,
          "recall": 0.9403533568904594,
          "f1-score": 0.9692599067599068,
          "support": 7075.0
        },
        "micro avg": {
          "precision": 0.9791110375954063,
          "recall": 0.9275385028653295,
          "f1-score": 0.9526272832821786,
          "support": 44672.0
        },
        "macro avg": {
          "precision": 0.9899230550014249,
          "recall": 0.9161919105883458,
          "f1-score": 0.9500781330324477,
          "support": 44672.0
        },
        "weighted avg": {
          "precision": 0.9804030579165649,
          "recall": 0.9275385028653295,
          "f1-score": 0.9517315928895655,
          "support": 44672.0
        },
        "samples avg": {
          "precision": 0.9838095238095238,
          "recall": 0.9407142857142857,
          "f1-score": 0.955079365079365,
          "support": 44672.0
        }
      },
      "confusion_matrix": null
    }
  ],
  "val_metrics": [
    {
      "accuracy": 0.877948717948718,
      "f1_score": 0.9487092547643632,
      "precision": 0.9886298139900445,
      "recall": 0.9150062321518831,
      "loss": 0.1132905519328238,
      "classification_report": {
        "legal_aid": {
          "precision": 0.9431490699502227,
          "recall": 0.9579563597658329,
          "f1-score": 0.9504950495049505,
          "support": 3758.0
        },
        "family_law": {
          "precision": 1.0,
          "recall": 0.819128171763175,
          "f1-score": 0.9005722460658083,
          "support": 1537.0
        },
        "consumer_protection": {
          "precision": 1.0,
          "recall": 0.8552036199095022,
          "f1-score": 0.9219512195121952,
          "support": 1326.0
        },
        "employment_law": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 1474.0
        },
        "fundamental_rights": {
          "precision": 1.0,
          "recall": 0.9427430093209055,
          "f1-score": 0.9705277587388622,
          "support": 1502.0
        },
        "micro avg": {
          "precision": 0.9761538461538461,
          "recall": 0.925601750547046,
          "f1-score": 0.9502059153874953,
          "support": 9597.0
        },
        "macro avg": {
          "precision": 0.9886298139900445,
          "recall": 0.9150062321518831,
          "f1-score": 0.9487092547643632,
          "support": 9597.0
        },
        "weighted avg": {
          "precision": 0.9777382728845406,
          "recall": 0.925601750547046,
          "f1-score": 0.9492945658999367,
          "support": 9597.0
        },
        "samples avg": {
          "precision": 0.9814529914529915,
          "recall": 0.938974358974359,
          "f1-score": 0.9531339031339031,
          "support": 9597.0
        }
      },
      "confusion_matrix": null
    },
    {
      "accuracy": 0.877948717948718,
      "f1_score": 0.9487092547643632,
      "precision": 0.9886298139900445,
      "recall": 0.9150062321518831,
      "loss": 0.10949396343820327,
      "classification_report": {
        "legal_aid": {
          "precision": 0.9431490699502227,
          "recall": 0.9579563597658329,
          "f1-score": 0.9504950495049505,
          "support": 3758.0
        },
        "family_law": {
          "precision": 1.0,
          "recall": 0.819128171763175,
          "f1-score": 0.9005722460658083,
          "support": 1537.0
        },
        "consumer_protection": {
          "precision": 1.0,
          "recall": 0.8552036199095022,
          "f1-score": 0.9219512195121952,
          "support": 1326.0
        },
        "employment_law": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 1474.0
        },
        "fundamental_rights": {
          "precision": 1.0,
          "recall": 0.9427430093209055,
          "f1-score": 0.9705277587388622,
          "support": 1502.0
        },
        "micro avg": {
          "precision": 0.9761538461538461,
          "recall": 0.925601750547046,
          "f1-score": 0.9502059153874953,
          "support": 9597.0
        },
        "macro avg": {
          "precision": 0.9886298139900445,
          "recall": 0.9150062321518831,
          "f1-score": 0.9487092547643632,
          "support": 9597.0
        },
        "weighted avg": {
          "precision": 0.9777382728845406,
          "recall": 0.925601750547046,
          "f1-score": 0.9492945658999367,
          "support": 9597.0
        },
        "samples avg": {
          "precision": 0.9814529914529915,
          "recall": 0.938974358974359,
          "f1-score": 0.9531339031339031,
          "support": 9597.0
        }
      },
      "confusion_matrix": null
    },
    {
      "accuracy": 0.877948717948718,
      "f1_score": 0.9487092547643632,
      "precision": 0.9886298139900445,
      "recall": 0.9150062321518831,
      "loss": 0.10940756779189521,
      "classification_report": {
        "legal_aid": {
          "precision": 0.9431490699502227,
          "recall": 0.9579563597658329,
          "f1-score": 0.9504950495049505,
          "support": 3758.0
        },
        "family_law": {
          "precision": 1.0,
          "recall": 0.819128171763175,
          "f1-score": 0.9005722460658083,
          "support": 1537.0
        },
        "consumer_protection": {
          "precision": 1.0,
          "recall": 0.8552036199095022,
          "f1-score": 0.9219512195121952,
          "support": 1326.0
        },
        "employment_law": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 1474.0
        },
        "fundamental_rights": {
          "precision": 1.0,
          "recall": 0.9427430093209055,
          "f1-score": 0.9705277587388622,
          "support": 1502.0
        },
        "micro avg": {
          "precision": 0.9761538461538461,
          "recall": 0.925601750547046,
          "f1-score": 0.9502059153874953,
          "support": 9597.0
        },
        "macro avg": {
          "precision": 0.9886298139900445,
          "recall": 0.9150062321518831,
          "f1-score": 0.9487092547643632,
          "support": 9597.0
        },
        "weighted avg": {
          "precision": 0.9777382728845406,
          "recall": 0.925601750547046,
          "f1-score": 0.9492945658999367,
          "support": 9597.0
        },
        "samples avg": {
          "precision": 0.9814529914529915,
          "recall": 0.938974358974359,
          "f1-score": 0.9531339031339031,
          "support": 9597.0
        }
      },
      "confusion_matrix": null
    },
    {
      "accuracy": 0.877948717948718,
      "f1_score": 0.9487092547643632,
      "precision": 0.9886298139900445,
      "recall": 0.9150062321518831,
      "loss": 0.10812464590461751,
      "classification_report": {
        "legal_aid": {
          "precision": 0.9431490699502227,
          "recall": 0.9579563597658329,
          "f1-score": 0.9504950495049505,
          "support": 3758.0
        },
        "family_law": {
          "precision": 1.0,
          "recall": 0.819128171763175,
          "f1-score": 0.9005722460658083,
          "support": 1537.0
        },
        "consumer_protection": {
          "precision": 1.0,
          "recall": 0.8552036199095022,
          "f1-score": 0.9219512195121952,
          "support": 1326.0
        },
        "employment_law": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 1474.0
        },
        "fundamental_rights": {
          "precision": 1.0,
          "recall": 0.9427430093209055,
          "f1-score": 0.9705277587388622,
          "support": 1502.0
        },
        "micro avg": {
          "precision": 0.9761538461538461,
          "recall": 0.925601750547046,
          "f1-score": 0.9502059153874953,
          "support": 9597.0
        },
        "macro avg": {
          "precision": 0.9886298139900445,
          "recall": 0.9150062321518831,
          "f1-score": 0.9487092547643632,
          "support": 9597.0
        },
        "weighted avg": {
          "precision": 0.9777382728845406,
          "recall": 0.925601750547046,
          "f1-score": 0.9492945658999367,
          "support": 9597.0
        },
        "samples avg": {
          "precision": 0.9814529914529915,
          "recall": 0.938974358974359,
          "f1-score": 0.9531339031339031,
          "support": 9597.0
        }
      },
      "confusion_matrix": null
    }
  ],
  "best_f1_score": 0.9487092547643632,
  "final_epoch": 4,
  "model_config": {
    "base_model": "distilbert-base-uncased",
    "max_length": 512,
    "batch_size": 32,
    "learning_rate": 2e-05,
    "epochs": 30,
    "warmup_steps": 200,
    "weight_decay": 0.01,
    "dropout_prob": 0.3,
    "num_domains": 5,
    "hidden_size": 768
  }
}